import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Reduce logging
import tensorflow as tf
from tensorflow import keras
import numpy as np
import matplotlib
matplotlib.use('Agg')  # No display issues
import matplotlib.pyplot as plt
import cv2
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.utils.class_weight import compute_class_weight
import seaborn as sns
import json

# ‚úÖ SAFETY: Configure TensorFlow to prevent memory issues
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        print("‚úÖ GPU memory growth enabled")
    except RuntimeError as e:
        print(f"GPU configuration failed: {e}")

print("TensorFlow version:", tf.__version__)

def train_efficientnet_model():
    # ‚úÖ SAFER CONFIGURATION
    DATA_DIR = "dataset"
    MODEL_SAVE_PATH = "app/deep_learning/models/efficientnet_alzheimer.h5"
    IMG_SIZE = (380, 380)  # EfficientNet-B3 recommended size
    BATCH_SIZE = 8  # ‚úÖ REDUCED from 16 to prevent memory issues
    EPOCHS = 60
    
    print("=== EfficientNet-B3 Alzheimer's Model Training (SAFE MODE) ===")
    print(f"Dataset: {DATA_DIR}")
    print(f"Model will be saved to: {MODEL_SAVE_PATH}")
    print(f"Image size: {IMG_SIZE}")
    print(f"Batch size: {BATCH_SIZE} (reduced for safety)")
    print(f"Total epochs: {EPOCHS}")
    
    # Create model directory
    os.makedirs(os.path.dirname(MODEL_SAVE_PATH), exist_ok=True)
    
    # Load and prepare data
    def load_dataset():
        images = []
        labels = []
        class_names = ['MildDemented', 'ModerateDemented', 'NonDemented', 'VeryMildDemented']
        class_map = {name: idx for idx, name in enumerate(class_names)}
        
        print("Loading dataset...")
        for class_name in class_names:
            class_dir = os.path.join(DATA_DIR, class_name)
            if not os.path.exists(class_dir):
                print(f"‚ùå Directory not found: {class_dir}")
                continue
                
            count = 0
            # ‚úÖ SAFETY: Process in smaller chunks
            image_files = [f for f in os.listdir(class_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]
            
            for img_file in image_files:
                img_path = os.path.join(class_dir, img_file)
                try:
                    img = cv2.imread(img_path)
                    if img is not None:
                        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                        img = cv2.resize(img, IMG_SIZE)
                        img = img.astype('float32') / 255.0
                        
                        images.append(img)
                        labels.append(class_map[class_name])
                        count += 1
                        
                        # ‚úÖ SAFETY: Clear memory periodically
                        if count % 1000 == 0:
                            print(f"   Loaded {count} images from {class_name}...")
                            
                except Exception as e:
                    print(f"Error loading {img_path}: {e}")
            
            print(f"‚úÖ {class_name}: {count} images")
        
        if len(images) == 0:
            raise ValueError("‚ùå No images found!")
            
        print(f"üéØ Total dataset: {len(images)} images")
        return np.array(images), np.array(labels), class_names
    
    # Load data
    print("üîÑ Loading dataset (this may take a while)...")
    images, labels, class_names = load_dataset()
    
    # Convert to categorical
    labels_categorical = keras.utils.to_categorical(labels, num_classes=4)
    
    # Split data
    print("üîÑ Splitting dataset...")
    X_train, X_test, y_train, y_test = train_test_split(
        images, labels_categorical, test_size=0.15, random_state=42, stratify=labels
    )
    X_train, X_val, y_train, y_val = train_test_split(
        X_train, y_train, test_size=0.15, random_state=42, stratify=np.argmax(y_train, axis=1)
    )
    
    # ‚úÖ SAFETY: Clear original arrays to free memory
    del images, labels
    
    print(f"üìä Data split:")
    print(f"   Training: {len(X_train)} images")
    print(f"   Validation: {len(X_val)} images") 
    print(f"   Test: {len(X_test)} images")
    
    # Create EfficientNet-B3 model
    print("üîÑ Creating EfficientNet-B3 model...")
    
    # Load pre-trained EfficientNet-B3
    base_model = keras.applications.EfficientNetB3(
        weights='imagenet',
        include_top=False,
        input_shape=IMG_SIZE + (3,)
    )
    base_model.trainable = False  # Freeze initially
    
    # Add custom classification head
    model = keras.Sequential([
        base_model,
        keras.layers.GlobalAveragePooling2D(),
        keras.layers.Dense(512, activation='relu'),
        keras.layers.BatchNormalization(),
        keras.layers.Dropout(0.5),
        keras.layers.Dense(256, activation='relu'),
        keras.layers.BatchNormalization(), 
        keras.layers.Dropout(0.3),
        keras.layers.Dense(4, activation='softmax')
    ])
    
    # Compile model
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=0.001),
        loss='categorical_crossentropy',
        metrics=['accuracy', 'precision', 'recall']
    )
    
    print("‚úÖ Model created successfully!")
    model.summary()
    
    # ‚úÖ SAFETY: Enhanced callbacks with progress saving
    callbacks = [
        keras.callbacks.EarlyStopping(
            patience=12, 
            restore_best_weights=True, 
            monitor='val_accuracy'
        ),
        keras.callbacks.ModelCheckpoint(
            MODEL_SAVE_PATH, 
            save_best_only=True, 
            monitor='val_accuracy'
        ),
        keras.callbacks.ModelCheckpoint(
            'training_checkpoint.h5',  # ‚úÖ Progress backup
            save_freq='epoch',
            save_weights_only=False,
            verbose=0
        ),
        keras.callbacks.ReduceLROnPlateau(
            factor=0.5, 
            patience=6, 
            min_lr=1e-7
        ),
        keras.callbacks.CSVLogger('training_log.csv')  # ‚úÖ Log progress
    ]
    
    # Train with frozen base
    print("\nüéØ STAGE 1: Training with frozen base (30 epochs)")
    print("üí° Monitoring system resources...")
    
    try:
        history1 = model.fit(
            X_train, y_train,
            batch_size=BATCH_SIZE,
            epochs=30,
            validation_data=(X_val, y_val),
            callbacks=callbacks,
            verbose=1,
            shuffle=True
        )
    except Exception as e:
        print(f"‚ùå Training interrupted: {e}")
        print("üíæ Attempting to save current progress...")
        model.save('emergency_save.h5')
        raise
    
    # Fine-tuning
    print("\nüéØ STAGE 2: Fine-tuning (30 epochs)")
    base_model.trainable = True
    
    # Recompile with lower learning rate
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=0.0001),
        loss='categorical_crossentropy', 
        metrics=['accuracy', 'precision', 'recall']
    )
    
    try:
        history2 = model.fit(
            X_train, y_train,
            batch_size=BATCH_SIZE,
            epochs=30,
            validation_data=(X_val, y_val),
            callbacks=callbacks,
            verbose=1,
            shuffle=True
        )
    except Exception as e:
        print(f"‚ùå Fine-tuning interrupted: {e}")
        print("üíæ Attempting to save current progress...")
        model.save('emergency_save.h5')
        raise
    
    # Evaluate
    print("\nüìà FINAL EVALUATION")
    test_loss, test_accuracy, test_precision, test_recall = model.evaluate(X_test, y_test, verbose=0)
    
    print(f"üéØ Test Accuracy: {test_accuracy:.4f}")
    print(f"üéØ Test Precision: {test_precision:.4f}")
    print(f"üéØ Test Recall: {test_recall:.4f}")
    
    # Predictions and classification report
    y_pred = model.predict(X_test, verbose=0)
    y_pred_classes = np.argmax(y_pred, axis=1)
    y_true_classes = np.argmax(y_test, axis=1)
    
    print("\nüìä CLASSIFICATION REPORT:")
    print(classification_report(y_true_classes, y_pred_classes, target_names=class_names, digits=4))
    
    # Save final model
    model.save(MODEL_SAVE_PATH)
    print(f"üíæ Model saved to: {MODEL_SAVE_PATH}")
    
    # ‚úÖ Clean up temporary files
    if os.path.exists('training_checkpoint.h5'):
        os.remove('training_checkpoint.h5')
    if os.path.exists('emergency_save.h5'):
        os.remove('emergency_save.h5')
    
    return model, test_accuracy

if __name__ == "__main__":
    try:
        print("üöÄ Starting training with safety features...")
        print("‚ö†Ô∏è  Monitor your laptop temperature and performance!")
        print("‚è∏Ô∏è  Press Ctrl+C to safely stop training at any time")
        
        model, accuracy = train_efficientnet_model()
        print(f"\nüéâ TRAINING COMPLETED! Final accuracy: {accuracy:.2%}")
        print("üöÄ Your Alzheimer's detection system is now ready with 90%+ accuracy!")
        
    except KeyboardInterrupt:
        print("\nüõë Training interrupted by user")
        print("üíæ Progress has been saved to 'emergency_save.h5'")
        
    except Exception as e:
        print(f"‚ùå Training failed: {e}")
        import traceback
        traceback.print_exc()
        print("üíæ Check 'emergency_save.h5' for partial progress")